# vllm server recipe for MeluXina
# Recommended paths for MeluXina HPC cluster

scenario: "vllm-meluxina-test"
partition: "gpu"
account: "p200776"
qos: "default"

# HPC modules to load
modules:
  - "Apptainer"

orchestration:
  mode: "slurm"
  total_nodes: 2
  node_allocation:
    servers:
      nodes: 1
    clients:
      nodes: 1
    monitors:
      nodes: 1
  job_config:
    time_limit: "02:00:00"
    exclusive: true

resources:
  servers:
    gpus: 1
    cpus_per_task: 1
    mem_gb: 32
  clients:
    gpus: 1
    cpus_per_task: 2
    mem_gb: 16

workload:
  component: "inference"
  service: "vllm"
  clients_per_node: 10
  duration: "2m"
  warmup: "1m"
  model: "facebook/opt-125m"  # Small model for testing, can use larger models like "meta-llama/Llama-2-7b-hf"

servers:
  health_check:
    enabled: true
    timeout: 300
    interval: 5
    endpoint: "/health"  # vLLM uses /health endpoint
  service_config:
    # vLLM-specific configuration
    # Models are loaded at startup via --model parameter in container start command
    tensor_parallel_size: 1  # Number of GPUs for tensor parallelism
    max_model_len: 2048  # Maximum sequence length

artifacts:
  containers_dir: "/project/home/p200776/team8/containers/"
  service:
    path: "vllm_0_5_0.sif"
    remote: "docker://vllm/vllm:0.5.0"  # Official vLLM Docker image with OpenAI-compatible API
  python:
    path: "python_3_12_3_v2.sif"
    remote: "docker://python:3.12.3-slim"

binds:
  # vLLM stores downloaded models here (HuggingFace cache)
  # Use $PROJECT path for more storage quota
  - "/project/home/p200776/team8/.cache/huggingface:/root/.cache/huggingface:rw"

  # Optional: Scratch space for temporary files
  - "/project/home/p200776/team8/:/scratch:rw"